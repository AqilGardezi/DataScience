{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-29 12:49:39.524549: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-29 12:49:39.524605: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt \n",
    "import pymssql\n",
    "# import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import seaborn as sns\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_data():\n",
    "#     df=pd.DataFrame()\n",
    "#     try:\n",
    "#         con = pymssql.connect(user='ds_rcm',password = 'mtbc@123'\n",
    "#                      ,host='172.16.0.32',database='mis_db',autocommit = True)\n",
    "#     except Exception as e:\n",
    "#         print('unable to make connection',e)\n",
    "#     cur=con.cursor()\n",
    "\n",
    "#     query = \"\"\"\n",
    "    \n",
    "#         IF OBJECT_ID('TEMPDB..#CLAIMS') IS NOT NULL\n",
    "#         DROP TABLE #CLAIMS\n",
    "#         SELECT CONVERT(DATE,C.DOS) AS DOS,CLAIM_NO INTO #CLAIMS FROM CLAIMS_SUBMITTED AS C\n",
    "#         JOIN PATIENT AS P ON P.PATIENT_ACCOUNT=C.PATIENT_ACCOUNT\n",
    "#         JOIN PRACTICES AS PR ON PR.PRACTICE_CODE=P.PRACTICE_CODE\n",
    "#         AND PR.IS_ACTIVE=1 AND ISNULL(PR.IS_TEST_PRACTICE,0)=0\n",
    "#         AND ISNULL(PR.EXCLUDE_FROM_BILLING_REPORTS,0) = 0\n",
    "#         AND PR.EMR_NAME NOT IN ('CHARTS PRO','PM STANDALONE','TESTINGPRACTICE')\n",
    "#         AND PR.PRACTICE_CODE NOT LIKE '9090%' AND PR.PRAC_NAME NOT LIKE '%TEST%'\n",
    "#         WHERE ISNULL(C.DELETED,0)=0 AND CONVERT(DATE,C.DOS) BETWEEN '01-01-2021' AND '10-20-2022'----6383824\n",
    "\n",
    "\n",
    "\n",
    "# #         ---------------------------#CHARGES----------------------\n",
    "# #         IF OBJECT_ID('TEMPDB..#CHARGES') IS NOT NULL\n",
    "# #         DROP TABLE #CHARGES\n",
    "# #         SELECT C.Claim_No,CC.claim_charges_id,CC.Amount INTO #CHARGES FROM Claim_Charges AS CC\n",
    "# #         JOIN #CLAIMS AS C ON CC.Claim_No=C.Claim_No\n",
    "# #         WHERE ISNULL(CC.DELETED,0)=0------\n",
    "\n",
    "\n",
    "\n",
    "# #         ---------------------------------------------------------\n",
    "# #         SELECT CONVERT(DATE,C.DOS)AS DOS,COUNT(DISTINCT C.Claim_No) AS CLAIM_COUNT,SUM(CC.Amount) AS CHARGES_SUM FROM #CLAIMS AS C LEFT JOIN #CHARGES AS CC\n",
    "# #         ON C.Claim_No=CC.Claim_No\n",
    "# #         GROUP BY CONVERT(DATE,C.DOS)\n",
    "# #         ORDER BY CONVERT(DATE,C.DOS) ASC;\n",
    "#         \"\"\"\n",
    "#     try:\n",
    "#         cur.execute(query)\n",
    "#         df=pd.DataFrame(cur.fetchall(),columns=[x[0] for x in cur.description])\n",
    "# #         print(df)\n",
    "# #         df.columns=df.columns.str.upper()\n",
    "#         con.close()\n",
    "#     except:\n",
    "#         print('Error in reading data.....')\n",
    "#         con.close()\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    df=pd.DataFrame()\n",
    "    try:\n",
    "        con = pymssql.connect(user='ds_rcm',password = 'mtbc@123'\n",
    "                     ,host='172.16.0.32',database='mis_db',autocommit = True)\n",
    "    except Exception as e:\n",
    "        print('unable to make connection',e)\n",
    "    cur=con.cursor()\n",
    "\n",
    "    query = \"\"\"\n",
    "    \n",
    "SELECT CONVERT(DATE,CS.Process_Date) AS PROCESS_DATE,COUNT(DISTINCT CS.CLAIM_NO) CLAIM_COUNT\n",
    "FROM CLAIMS_SUBMITTED AS CS\n",
    "JOIN CLAIMS AS C ON C.Claim_No=CS.Claim_No\n",
    "JOIN PATIENT AS P ON P.PATIENT_ACCOUNT=C.PATIENT_ACCOUNT\n",
    "JOIN PRACTICES AS PR ON PR.PRACTICE_CODE=P.PRACTICE_CODE\n",
    "AND PR.IS_ACTIVE=1 AND ISNULL(PR.IS_TEST_PRACTICE,0)=0\n",
    "AND ISNULL(PR.EXCLUDE_FROM_BILLING_REPORTS,0) = 0\n",
    "AND PR.EMR_NAME NOT IN ('CHARTS PRO','PM STANDALONE','TESTINGPRACTICE')\n",
    "AND PR.PRACTICE_CODE NOT LIKE '9090%' AND PR.PRAC_NAME NOT LIKE '%TEST%'\n",
    "WHERE ISNULL(CS.DELETED,0)=0 AND CONVERT(DATE,CS.Process_Date) BETWEEN '01-01-2020' AND '06-20-2022'\n",
    "GROUP BY CONVERT(DATE,CS.Process_Date)\n",
    "ORDER BY CONVERT(DATE,CS.Process_Date) ASC\n",
    "        \"\"\"\n",
    "    try:\n",
    "        cur.execute(query)\n",
    "        df=pd.DataFrame(cur.fetchall(),columns=[x[0] for x in cur.description])\n",
    "#         print(df)\n",
    "#         df.columns=df.columns.str.upper()\n",
    "        con.close()\n",
    "    except:\n",
    "        print('Error in reading data.....')\n",
    "        con.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims.to_csv('Submission_Claims_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims = pd.read_csv('Submission_Claims_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_claims = claims['CLAIM_COUNT'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims['CLAIM_COUNT'].mask( claims['CLAIM_COUNT'] >= 40000, 40000 , inplace=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_claims = claims.loc[ claims['CLAIM_COUNT'] >= 50000, 'CLAIM_COUNT'] = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims = claims[[\"PROCESS_DATE\", \"CLAIM_COUNT\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims[\"PROCESS_DATE\"] = pd.to_datetime(claims[\"PROCESS_DATE\"])\n",
    "claims = claims.sort_values('PROCESS_DATE')\n",
    "claims =claims.set_index(\"PROCESS_DATE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# claims.loc[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims.plot(figsize=(20,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec= seasonal_decompose(claims['CLAIM_COUNT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = claims.loc[:'2022-04-30']\n",
    "test = claims.loc['2022-05-01':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train = scaler.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_test = scaler.transform(test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(scaled_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define generator\n",
    "n_input = 32\n",
    "n_features = 1\n",
    "generator = TimeseriesGenerator(scaled_train, scaled_train, length=n_input, batch_size=16\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(units =128, activation='relu', return_sequences=True, input_shape = (n_input, n_features)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units =128, activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units =128, activation='relu', return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=1)) # Prediction of the next value\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "model.fit(generator,epochs=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_per_epoch = model.history.history['loss']\n",
    "plt.plot(range(len(loss_per_epoch)),loss_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_eval_batch = scaled_train[-32:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_eval_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_eval_batch = first_eval_batch.reshape((1,n_input,n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(first_eval_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#holding my predictions\n",
    "test_predictions = []\n",
    "\n",
    "\n",
    "# last n_input points from the training set\n",
    "first_eval_batch = scaled_train[-n_input:]\n",
    "# reshape this to the format RNN wants (same format as TimeseriesGeneration)\n",
    "current_batch = first_eval_batch.reshape((1,n_input,n_features))\n",
    "\n",
    "#how far into the future will I forecast?\n",
    "\n",
    "for i in range(len(test)):\n",
    "    \n",
    "    # One timestep ahead of historical 12 points\n",
    "    current_pred = model.predict(current_batch)[0]\n",
    "    \n",
    "    #store that prediction\n",
    "    test_predictions.append(current_pred)\n",
    "    \n",
    "    # UPDATE current batch o include prediction\n",
    "    current_batch = np.append(current_batch[:,1:,:],[[current_pred]], axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_predictions = scaler.inverse_transform(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Predictions'] =true_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accu = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test.plot(figsize=(12,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "rmse=sqrt(mean_squared_error(test['CLAIM_COUNT'],test['Predictions']))\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    \"\"\"Calculates MAPE given y_true and y_pred\"\"\"\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(test['CLAIM_COUNT'],test['Predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aaqil_env",
   "language": "python",
   "name": "aaqil_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
